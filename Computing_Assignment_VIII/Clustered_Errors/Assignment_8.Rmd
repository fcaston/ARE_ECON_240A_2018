---
title: "Assignment_8"
author: "Clustered Errors"
date: "3/14/2018"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(MASS)
library(broom)
library(modelr)
library(tidyverse)
select <- dplyr::select
```

```{r sim}
sim_reg <- function(n=100, B=200) {
  Sigma <-matrix(c(1, 0.7, 0.7, 1), 2)
  X <-mvrnorm(n=n, c(0, 0), Sigma = Sigma)
  y <- 1.2 + 0.3*X[,1]+1.1*X[,2] + rnorm(n)
  df <-data_frame(y=y, X1=X[,1], X2=X[,2])
  
  ## reg
  reg <-lm(y~X1+X2, data=df)
  reg_su <-summary(reg)
  reg_co <-tidy(reg_su)
  co_X0 <- reg_co$estimate[1]
  co_X1 <- reg_co$estimate[2]
  co_X2 <- reg_co$estimate[3]
  
  theta_hat <- 1.1*co_X1 - 0.3*co_X2
  
  ## Delta
  # R <- COMPUTE IT for the three parameters
  R <- matrix(c(0, 1.1, -0.3),3)
  v_theta <- as.numeric(t(R) %*% vcov(reg_su) %*% R)

  
  # W-test
  W_test <- ((theta_hat)^2)/v_theta
  
  ## boot it:
  x_boot <- modelr::bootstrap(df, n=B) %>% 
    mutate(reg = map(strap, ~lm(y~X1+X2, data=.)),
           reg_su = map(reg, summary),
           reg_co = map(reg_su, tidy),
           theta_b = map_dbl(reg_co, ~ 1.1 * .$estimate[2] - 0.3 * .$estimate[3]),
           vcov_b = map_dbl(reg_su, ~ t(R) %*% vcov(.) %*% R),
           W_test_b = map2_dbl(theta_b, vcov_b, ~ ((.x)^2)/.y),
           W_test_b2 = map2_dbl(theta_b, vcov_b, ~ ((.x - theta_hat)^2)/.y))
  x_boot2 <- x_boot %>% 
    summarise(boot_sd = sd(theta_b),
              boot_sd_v = mean(vcov_b),
              boot_mean = mean(theta_b),
              wtest_cval = quantile(W_test_b, probs = c(0.95)),
              wtest2_cval = quantile(W_test_b2, probs = c(0.95)))
  
  ## results: original Wald and va, and boot aggregates
  data_frame(co_X1 = co_X1, co_X2 = co_X2, theta = theta_hat, W_test = W_test, var = v_theta) %>% 
    bind_cols(x_boot2)
  }
```

```{r}
results <- sim_reg()
```


# Cross Validation

```{r cross_validation}
# Generate Data
n <- 100
S <- matrix(c(1, 0.5, 0.5, 1), 2)
set.seed(2018)
X <- mvrnorm(n = n, mu = c(0, 0), Sigma = S)
y <- 1.2 + 0.3 * X[, 1] + rnorm(n)

data_df <- data_frame(y = y, x1 = X[, 1], x2 = X[, 2])

# Perform Cross Validation
cv_df <- crossv_kfold(data_df, k = n)

crossval <- cv_df %>%
  mutate(reg1 = map(train, ~ lm(y ~ x1, data = .)),
         reg2 = map(train, ~ lm(y ~ x1 + x2, data = .))) %>%
  gather(model, reg, reg1, reg2) %>%
  mutate(resids_out = map2_dbl(reg, test, ~ predict(.x, newdata = .y) - as_data_frame(.y)$y),
         mse_in = map_dbl(reg, ~ mean(residuals(.)^2)),
         mse_out = map_dbl(resids_out, ~ mean(.^2))) %>%
  gather(mse, value, mse_in, mse_out)
```

## Cross Validation: The Crossval Data Frame

The Cross Validation data frame is a $100 \times 3$ data frame, with 100 rows (corresponding to the number of observations in the sample) and three variables. The first variable is the training set (the sample containing the 99 observations not removed) and the second is the testing set (the one observation removed). The objects contained in both of these variables are themselves data frames (though they are stored as lists to manage storage space). The lists in train and test are length 2, but the data frames contained within are also $100 \times 3$. 

## Cross Validation: Output Data Frame

The output data frame contains the results of the regressions and the calculations of mean squared error. The _MSE_ column contains character strings that refer to whether the mean squared error value contained in _value_ is from the in-sample estimation or out-of-sample estimation. Similarly, the _model_ variable simply is an indicator for whether the regression results are from the regression of $y$ on $x_1$ alone or from $y$ on $x_1$ and $x_2$. 

The variable `resids_out` is calculated by using the estimated regression model from the training set to predict values using the one observation left out of the sample (the testing set). Essentially, the `map_dbl` function takes values from the variables `reg` and `test`, which are are regression output and a data frame respectively, uses the function `predict` to predict $\hat{y}$ values, and calculates the residuals as $\hat{y} - y$. In this case, `.x` is the regression output stored in `reg`, and `.y` is the testing set in `test`. 

```{r}

```


## Cross Validation: Summary of Results


